{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6efdf518",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# dbt Databricks demo\n",
    "\n",
    "\n",
    "Slides and example project available at  https://github.com/ConstantinoSchillebeeckx/dbt_databricks_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126c95b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# data build tool (dbt)\n",
    "\n",
    "* https://www.getdbt.com/\n",
    "* SQL + Jinja (and now Python!)\n",
    "* lineage\n",
    "* data testing\n",
    "* docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80743ff3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# docs\n",
    "\n",
    "```\n",
    "dbt docs generate\n",
    "dbt docs serve\n",
    "```\n",
    "\n",
    "http://127.0.0.1:8080/#!/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcba712",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# config\n",
    "\n",
    "* project file (e.g. dbt_project.yml)\n",
    "* property file (e.g. schema.yml)\n",
    "* config block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae8127",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "name: 'e2'\n",
    "version: '1.0.0'\n",
    "config-version: 2\n",
    "\n",
    "# This setting configures which \"profile\" dbt uses for this project.\n",
    "profile: 'e2'\n",
    "\n",
    "# Configuring models\n",
    "models:\n",
    "  +persist_docs:\n",
    "    relation: true\n",
    "    columns: true\n",
    "\n",
    "  e2:\n",
    "    +materialized: table\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71fac2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SQL model\n",
    "\n",
    "By default work is submitted as an \"execution context\" to either a SQLWarehouse cluster or an all-purpose cluster; here we change the default so that the model is executed as a notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ea7b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**e2/models/zip_ny.sql**\n",
    "\n",
    "```\n",
    "{{ config(create_notebook=True) }}\n",
    "\n",
    "SELECT *\n",
    "FROM {{ ref('uszips') }}\n",
    "WHERE state_id = \"NY\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669addaa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SQL model config\n",
    "\n",
    "**e2/model/zip_ny.yaml**\n",
    "\n",
    "```\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: zip_ny\n",
    "    description: Zipcodes limited to NY\n",
    "    columns:\n",
    "      - name: zip\n",
    "        description: The zipcode\n",
    "        tests:\n",
    "          - not_null\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd61ca5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python model\n",
    "\n",
    "**e2/models/zip_co.py**\n",
    "\n",
    "```\n",
    "def model(dbt, session):\n",
    "    df = dbt.ref(\"uszips\")\n",
    "    return df.filter(df.state_id == \"CO\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35020a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* `dbt`: A class compiled by dbt Core, unique to each model, enables you to run your Python code in the context of your dbt project and DAG.\n",
    "* `session`: A class representing your data platformâ€™s connection to the Python backend. The session is needed to read in tables as DataFrames, and to write DataFrames back to tables. In PySpark, by convention, the SparkSession is named spark, and available globally. For consistency across platforms, we always pass it into the model function as an explicit argument called session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14afef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python model config\n",
    "\n",
    "**e2/models/zip_co.py**\n",
    "\n",
    "```\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: zip_co\n",
    "    description: Zipcodes limited to CO\n",
    "    columns:\n",
    "      - name: zip\n",
    "        description: The zipcode\n",
    "        tests:\n",
    "          - not_null\n",
    "```          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b514aa14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python model\n",
    "\n",
    "Models can be customized to execute in their own cluster; like all configurations, this can be done at the model level, in a propery file or at a project level.\n",
    "\n",
    "**e2/models/zip_pr.py**\n",
    "\n",
    "```\n",
    "def model(dbt, session):\n",
    "    dbt.config(\n",
    "        submission_method=\"job_cluster\",\n",
    "        job_cluster_config={\n",
    "            \"spark_version\": \"11.3.x-scala2.12\",\n",
    "            \"node_type_id\": \"i3.xlarge\",\n",
    "            \"num_workers\": 0,\n",
    "            \"spark_conf\": {\n",
    "                \"spark.databricks.cluster.profile\": \"singleNode\",\n",
    "                \"spark.master\": \"local[*, 4]\",\n",
    "            },\n",
    "            \"custom_tags\": {\"ResourceClass\": \"SingleNode\"},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    df = dbt.ref(\"uszips\")\n",
    "\n",
    "    return df.filter(df.state_id == \"PR\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f07b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Selecting nodes\n",
    "\n",
    "* `dbt ls --model zip_co+` select `zip_co` and all children (downstream) models\n",
    "* `dbt ls --model +join` select `join` and all parent (upstream) models\n",
    "* `dbt ls --model @zip_co` select `zip` and all children and parents of those children\n",
    "\n",
    "![dag](dag.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87bbcbe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's run!\n",
    "\n",
    "```\n",
    "dbt run\n",
    "dbt run --model model_name+3\n",
    "dbt run --select tag:prod\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02e6a13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How it *might* work\n",
    "\n",
    "* a single flow responsible for executing all dbt models\n",
    "* by default this flow does `dbt run` (execute all models)\n",
    "* flow accepts parameters so that we can easily rerun failed models like `dbt run failed_model+`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3fe122",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Limitations\n",
    "\n",
    "* EMR? this should work, haven't tried it yet, there is a `dbt-spark` adapter\n",
    "* data tests can only be written in SQL out of the box\n",
    "* SQL and Python models don't have parity\n",
    "  * cannot declare a custom S3 location for Python models (SQL supports `location_root`), there's an [GH issue](https://github.com/dbt-labs/dbt-spark/issues/559) for this\n",
    "  * table comments not materializing for Python models, through column level comments do"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
