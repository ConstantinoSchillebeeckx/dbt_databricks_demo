{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6efdf518",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# dbt Databricks demo\n",
    "\n",
    "\n",
    "Slides and example project available at  https://github.com/ConstantinoSchillebeeckx/dbt_databricks_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126c95b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# data build tool (dbt)\n",
    "\n",
    "* https://www.getdbt.com/\n",
    "* SQL + Jinja (and now Python!)\n",
    "* lineage\n",
    "* data testing\n",
    "* docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80743ff3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# docs\n",
    "\n",
    "```bash\n",
    "dbt docs generate\n",
    "dbt docs serve\n",
    "```\n",
    "\n",
    "http://127.0.0.1:8080/#!/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcba712",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# config\n",
    "\n",
    "* project file (e.g. dbt_project.yml)\n",
    "* property file (e.g. schema.yml)\n",
    "* config block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b80d73",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```yaml\n",
    "name: 'e2'\n",
    "version: '1.0.0'\n",
    "config-version: 2\n",
    "\n",
    "# This setting configures which \"profile\" dbt uses for this project.\n",
    "profile: 'e2'\n",
    "\n",
    "# Configuring models\n",
    "models:\n",
    "  +persist_docs:\n",
    "    relation: true\n",
    "    columns: true\n",
    "\n",
    "  e2:\n",
    "    +materialized: table\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71fac2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SQL model\n",
    "\n",
    "By default work is submitted as an \"execution context\" to either a SQLWarehouse cluster or an all-purpose cluster.\n",
    "\n",
    "**e2/models/zip_ny.sql**\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM {{ ref('uszips') }}\n",
    "WHERE state_id = \"NY\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669addaa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SQL model config\n",
    "\n",
    "**e2/model/zip_ny.yaml**\n",
    "\n",
    "```yaml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: zip_ny\n",
    "    description: Zipcodes limited to NY\n",
    "    columns:\n",
    "      - name: zip\n",
    "        description: The zipcode\n",
    "        tests:\n",
    "          - not_null\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd61ca5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python model\n",
    "\n",
    "Here we configure dbt to execute the model as a Notebook.\n",
    "\n",
    "**e2/models/zip_co.py**\n",
    "\n",
    "```python\n",
    "def model(dbt, session):\n",
    "    dbt.config(create_notebook=True)\n",
    "\n",
    "    df = dbt.ref(\"uszips\")\n",
    "    return df.filter(df.state_id == \"CO\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35020a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* `dbt`: A class compiled by dbt Core, unique to each model, enables you to run your Python code in the context of your dbt project and DAG.\n",
    "* `session`: A class representing your data platformâ€™s connection to the Python backend. The session is needed to read in tables as DataFrames, and to write DataFrames back to tables. In PySpark, by convention, the SparkSession is named spark, and available globally. For consistency across platforms, we always pass it into the model function as an explicit argument called session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14afef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python model config\n",
    "\n",
    "**e2/models/zip_co.py**\n",
    "\n",
    "```yaml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: zip_co\n",
    "    description: Zipcodes limited to CO\n",
    "    columns:\n",
    "      - name: zip\n",
    "        description: The zipcode\n",
    "        tests:\n",
    "          - not_null\n",
    "```          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b514aa14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python model\n",
    "\n",
    "Models can be customized to execute in their own cluster; like all configurations, this can be done at the model level, in a propery file or at a project level; specifying a custom cluster always submits work as a Notebook.\n",
    "\n",
    "**e2/models/zip_pr.py**\n",
    "\n",
    "```python\n",
    "def model(dbt, session):\n",
    "    dbt.config(\n",
    "        submission_method=\"job_cluster\",\n",
    "        job_cluster_config={\n",
    "            \"spark_version\": \"11.3.x-scala2.12\",\n",
    "            \"node_type_id\": \"i3.xlarge\",\n",
    "            \"num_workers\": 0,\n",
    "            \"spark_conf\": {\n",
    "                \"spark.databricks.cluster.profile\": \"singleNode\",\n",
    "                \"spark.master\": \"local[*, 4]\",\n",
    "            },\n",
    "            \"custom_tags\": {\"ResourceClass\": \"SingleNode\"},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    df = dbt.ref(\"uszips\")\n",
    "\n",
    "    return df.filter(df.state_id == \"PR\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f07b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Selecting nodes\n",
    "\n",
    "* `dbt ls --model zip_co+` select `zip_co` and all children (downstream) models\n",
    "* `dbt ls --model +join` select `join` and all parent (upstream) models\n",
    "* `dbt ls --model @zip_co` select `zip` and all children and parents of those children\n",
    "\n",
    "![dag](dag.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87bbcbe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's run!\n",
    "\n",
    "```bash\n",
    "dbt run\n",
    "dbt run --model model_name+3\n",
    "dbt run --select tag:prod\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02e6a13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How it *might* work\n",
    "\n",
    "* a single flow responsible for executing all dbt models\n",
    "* by default this flow does `dbt run` (execute all models)\n",
    "* flow accepts parameters so that we can easily rerun failed models like `dbt run failed_model+`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d91ddb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unit tests\n",
    "\n",
    "* [active discussion in dbt community](https://github.com/dbt-labs/dbt-core/discussions/4455) & [slack](https://getdbt.slack.com/archives/C03QUA7DWCW/p1659431685815919)\n",
    "* dbt-core contains [testing framework](https://docs.getdbt.com/guides/dbt-ecosystem/adapter-development/4-testing-a-new-adapter) for pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44616b5d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "if we roll our own unit testing ...\n",
    "\n",
    "```python\n",
    "from pytest import fixture\n",
    "from pandas.testing import assert_frame_equal\n",
    "from e2.models.zip_pr import model\n",
    "\n",
    "@fixture\n",
    "def dbt():\n",
    "    # not sure what this looks like ...\n",
    "    pass\n",
    "\n",
    "@fixture\n",
    "def dbt_input(dbt):\n",
    "    # pydantic model? read CSV?\n",
    "    \n",
    "    dbt.ref.input = something\n",
    "    \n",
    "    yield dbt\n",
    "\n",
    "def test_pr_model(dbt_input, local_spark_session):\n",
    "    \n",
    "    df_out = model(dbt_input, session=local_spark_session)\n",
    "    \n",
    "    assert_frame_equal(df_out, df_expected)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3fe122",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Limitations\n",
    "\n",
    "* EMR? this should work, haven't tried it yet, there is a `dbt-spark` adapter; you can also easily set dynamic backend targets, e.g. one for databricks and one for EMR\n",
    "* data tests can only be written in SQL out of the box\n",
    "* SQL and Python models don't have parity\n",
    "  * cannot declare a custom S3 location for Python models (SQL supports `location_root`), there's an [GH issue](https://github.com/dbt-labs/dbt-spark/issues/559) for this\n",
    "  * table comments not materializing for Python models, through column level comments do\n",
    "* does not support retry out of the box, one suggestion is found [here](https://github.com/PrefectHQ/prefect-recipes/tree/main/prefect-v1-legacy/use-cases/rerun_dbt_models_from_failure) though that requires access to persitent state"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
